{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils import MVTecDataset, MVTecActiveDataset, evaluate_accuracy, getFileList\n",
    "from models import MVTecCNN_BO\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-t', '--target', required=True, help='target class')\n",
    "parser.add_argument('-k', '--no_ensemble', default= 3, type=int, help='number of esemble models for active learning')\n",
    "parser.add_argument('--unlabel_ratio', default= 0.5, type=float, help='the ratio of unlabeled data')\n",
    "parser.add_argument('--sample_ratio', default= 0.1, type=float, help='the ratio of sampling from unlabeled pool')\n",
    "\n",
    "\n",
    "parser.add_argument('-c', '--no_cuda', required=False, default=None, help='which cuda')\n",
    "parser.add_argument('--lr', default= 0.001, type=float , required=False, help='learning rate')\n",
    "parser.add_argument('--no_epoch', default= 30, type= int, required=False, help='number of epochs')\n",
    "\n",
    "args = parser.parse_args(\"--target zipper -k 3 --unlabel_ratio 0.7 --sample_ratio 0.1 -c 3 --lr 0.001\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = args.target\n",
    "num_ensemble = args.no_ensemble\n",
    "sample_ratio =  args.sample_ratio\n",
    "unlabeled_ratio =  args.unlabel_ratio\n",
    "\n",
    "save_path = os.path.join('./saves_active/', target_class)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "activeBatchSize = 16\n",
    "best_accuracies = np.zeros(num_ensemble)\n",
    "best_models={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "device_type='cuda'\n",
    "\n",
    "if args.no_cuda is not None:\n",
    "    device_type = 'cuda:'+str(args.no_cuda)\n",
    "    \n",
    "device = torch.device(device_type if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadActiveDataset(target_class):\n",
    "    normal_list_dir = [os.path.join('./data/', target_class, 'train', 'good'), os.path.join('./data/', target_class, 'test', 'good')]\n",
    "\n",
    "    test_dir = os.path.join('./data/', target_class, 'test')\n",
    "    test_subfolders = next(os.walk(test_dir))[1]\n",
    "\n",
    "    abnormal_list_dir=[]\n",
    "    \n",
    "    for item in test_subfolders:\n",
    "        if item != 'good':\n",
    "            abnormal_list_dir.append(os.path.join('./data/', target_class, 'test', item))\n",
    "    \n",
    "    normal_file_list, abnormal_file_list = getFileList(normal_list_dir, abnormal_list_dir)\n",
    "    normal_rand_idx = np.random.permutation(len(normal_file_list))\n",
    "    abnormal_rand_idx = np.random.permutation(len(abnormal_file_list))\n",
    "    \n",
    "    unlabeled_normal_num = int(unlabeled_ratio*len(normal_file_list))\n",
    "    unlabeled_abnormal_num = int(unlabeled_ratio*len(abnormal_file_list))\n",
    "    \n",
    "    unlabeled_normal_data_list = normal_file_list[normal_rand_idx[:unlabeled_normal_num]]\n",
    "    unlabeled_abnormal_data_list = abnormal_file_list[abnormal_rand_idx[:unlabeled_abnormal_num]]\n",
    "    \n",
    "    labeled_normal_data_list = normal_file_list[normal_rand_idx[unlabeled_normal_num:]]\n",
    "    labeled_abnormal_data_list = abnormal_file_list[abnormal_rand_idx[unlabeled_abnormal_num:]]\n",
    "    \n",
    "    return labeled_normal_data_list, labeled_abnormal_data_list, unlabeled_normal_data_list, unlabeled_abnormal_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_normal_data_list, labeled_abnormal_data_list, unlabeled_normal_data_list, unlabeled_abnormal_data_list = loadActiveDataset(target_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(labeled_normal_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = MVTecActiveDataset(unlabeled_normal_data_list, unlabeled_abnormal_data_list, isUnlabeled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = MVTecActiveDataset(labeled_normal_data_list, labeled_abnormal_data_list, isUnlabeled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unlabeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_num = int(len(labeled_dataset)*0.18)\n",
    "test_num = int(len(labeled_dataset)*0.2)\n",
    "train_num = len(labeled_dataset)  - val_num - test_num\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset =random_split(labeled_dataset,[train_num, val_num, test_num])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "active_loader = DataLoader(unlabeled_dataset, batch_size=activeBatchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = {0:train_loader, 1:valid_loader, 2:test_loader, 3:active_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath_dataloader = os.path.join(save_path, f\"dataloader_target-{target_class}_ensemble-{num_ensemble}_unlabel-{unlabeled_ratio}_annotate-{sample_ratio}.pth\")\n",
    "torch.save(data_loaders, savepath_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_esemble_models(best_val_acc, net):\n",
    "    if np.min(best_accuracies) < best_val_acc:\n",
    "        idx = np.argmin(best_accuracies)\n",
    "        best_models[idx] = net\n",
    "        best_accuracies[idx] = best_val_acc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr, num_channel):        \n",
    "    net = MVTecCNN_BO(num_channel).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    best_val_acc = 0.    \n",
    "    num_epoch = 3\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        loss_count=0\n",
    "        loss_sum=0\n",
    "        for idx, (img, label) in enumerate(train_loader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device, dtype=torch.float)\n",
    "            label = label.view(-1,1)\n",
    "            pred = net(img)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum+=loss.item()\n",
    "            loss_count+=1\n",
    "            if idx%10==0:\n",
    "                val_acc = evaluate_accuracy(net, valid_loader, device)\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc                    \n",
    "                best_model = copy.deepcopy(net)\n",
    "        scheduler.step()\n",
    "        \n",
    "    save_esemble_models(best_val_acc, best_model.eval())\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_function(lr, num_channel):\n",
    "    num_channel = int(8 + num_channel*54)   # min 8, max 64\n",
    "    best_val_accuracy = train(lr, num_channel)    \n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "pbounds = {'lr': (1e-3, 0.1), 'num_channel':(0, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=cnn_function,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=5\n",
    ")\n",
    "end = time.time()\n",
    "print('Time consumed: ' , end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(8 + 0.29725476914104565*54)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- original CNN accuracy 0.7674 (lr: 0.001, num_channel: 32)\n",
    "- auto-hyperparameter tuning with BO 0.8139 (lr: 0.00116, num_channel: 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath_ensembles = os.path.join(save_path, f\"ensemble_models_target-{target_class}_ensemble-{num_ensemble}_unlabel-{unlabeled_ratio}_annotate-{sample_ratio}.pth\")\n",
    "torch.save(best_models, savepath_ensembles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Load state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath_dataloader = os.path.join(save_path, f\"dataloader_target-{target_class}_ensemble-{num_ensemble}_unlabel-{unlabeled_ratio}_annotate-{sample_ratio}.pth\")\n",
    "data_loaders = torch.load(savepath_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_loaders[0]\n",
    "valid_loader =  data_loaders[1]\n",
    "test_loader =  data_loaders[2]\n",
    "active_loader =  data_loaders[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = active_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath_ensembles = os.path.join(save_path, f\"ensemble_models_target-{target_class}_ensemble-{num_ensemble}_unlabel-{unlabeled_ratio}_annotate-{sample_ratio}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath_ensembles = os.path.join(save_path, f\"ensemble_models_target-{target_class}_ensemble-{num_ensemble}_unlabel-{unlabeled_ratio}_annotate-{sample_ratio}.pth\")\n",
    "best_models = torch.load(savepath_ensembles)\n",
    "\n",
    "best_models_cpu={}\n",
    "for i in best_models:\n",
    "    best_models_cpu[i] = best_models[i].cpu()\n",
    "\n",
    "torch.save(best_models_cpu, savepath_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models[2].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = torch.load(savepath_ensembles)\n",
    "best_model_num_channels=[]\n",
    "best_accuracies =[]\n",
    "for i in range(num_ensemble):\n",
    "    best_model_num_channels.append(best_models_cpu[i].num_channel)    \n",
    "    best_accuracies.append(evaluate_accuracy(best_models_cpu[i].eval(), valid_loader, device='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = best_models[np.argmax(best_accuracies)].cpu().eval()\n",
    "original_model_test_acc = evaluate_accuracy(original_model, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_model_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_cpu={}\n",
    "for i in best_models:\n",
    "    best_models_cpu[i] = best_models[i].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add annotation from unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentDataset(train_dataset, valid_dataset, unlabeled_dataset, idx):\n",
    "    active_data_list = np.array(unlabeled_dataset.data_list)[idx]\n",
    "    active_label_list = np.array(unlabeled_dataset.label_list)[idx]\n",
    "    \n",
    "    labeled_dataset.data_list = train_dataset.data_list + valid_dataset.data_list + active_data_list.tolist()\n",
    "    labeled_dataset.label_list = train_dataset.label_list + valid_dataset.label_list + active_label_list.tolist()\n",
    "\n",
    "    val_num = int(len(labeled_dataset)*0.20)\n",
    "    train_num = len(labeled_dataset)  - val_num\n",
    "\n",
    "    new_train_dataset, new_valid_dataset =random_split(labeled_dataset,[train_num, val_num])\n",
    "\n",
    "    new_train_loader = DataLoader(new_train_dataset, batch_size=8, shuffle=True)\n",
    "    new_valid_loader = DataLoader(new_valid_dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    return new_train_loader, new_valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (img, label) in enumerate(active_loader):\n",
    "    img = img.to(device)\n",
    "    label = label.to(device, dtype=torch.float)\n",
    "       \n",
    "    for model_idx in best_models:\n",
    "\n",
    "        best_models[model_idx]\n",
    "        label = label.view(-1,1)\n",
    "        if model_idx==0:\n",
    "            total_tensor = best_models[model_idx](img)\n",
    "        else:\n",
    "            total_tensor = torch.cat((total_tensor, best_models[model_idx](img)), dim=1)        \n",
    "    \n",
    "    p = torch.mean(total_tensor, dim=1)\n",
    "    H = -p*torch.log(p)\n",
    "    \n",
    "    if idx==0:\n",
    "        total_H = H\n",
    "    else:\n",
    "        total_H = torch.cat((total_H, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_num = int(len(unlabeled_dataset)*sample_ratio)\n",
    "print(query_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_vals, active_idx = torch.topk(total_H, query_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_idx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_train_loader, active_valid_loader = augmentDataset(train_dataset, valid_dataset, unlabeled_dataset, active_idx.cpu())\n",
    "\n",
    "rand_idx = np.random.permutation(len(unlabeled_dataset))[:query_num]\n",
    "normal_train_loader, normal_valid_loader = augmentDataset(train_dataset, valid_dataset, unlabeled_dataset, rand_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activeTrain(idx, lr, isActiveLearn):\n",
    "    \n",
    "    if isActiveLearn:\n",
    "        method='active'\n",
    "        train_loader = active_train_loader\n",
    "        valid_loader = active_valid_loader\n",
    "    else:\n",
    "        method='normal'\n",
    "        train_loader = normal_train_loader\n",
    "        valid_loader = normal_valid_loader\n",
    "    \n",
    "    net = copy.deepcopy(best_models[idx])\n",
    "    net.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    best_val_acc = 0.\n",
    "    \n",
    "    num_epoch = 20\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        loss_count=0\n",
    "        loss_sum=0\n",
    "        for idx, (img, label) in enumerate(train_loader):\n",
    "            img = img.to(device)\n",
    "            label = label.to(device, dtype=torch.float)\n",
    "            label = label.view(-1,1)\n",
    "            pred = net(img)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum+=loss.item()\n",
    "            loss_count+=1\n",
    "            if idx%10==0:\n",
    "                net.eval()\n",
    "                val_acc = evaluate_accuracy(net, valid_loader, device)\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model = copy.deepcopy(net)\n",
    "\n",
    "                net.train()\n",
    "        scheduler.step()   \n",
    "#     save_esemble_models(best_val_acc, net.eval())\n",
    "    return best_val_acc, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_best_active_acc = 0\n",
    "savepath_active = os.path.join(save_path, f\"active_target-{target_class}_ensemble-{num_ensemble}_unlabel-{unlabeled_ratio}_annotate-{sample_ratio}.pth\")\n",
    "\n",
    "def cnn_active_function(lr, model_index):    \n",
    "    global global_best_active_acc\n",
    "    if model_index==1:\n",
    "        idx=num_ensemble-1\n",
    "    else:\n",
    "        idx = int(model_index*num_ensemble)      \n",
    "    best_acc, best_model = activeTrain(idx, lr, isActiveLearn=True)\n",
    "    \n",
    "    if best_acc > global_best_active_acc:\n",
    "        global_best_active_acc = best_acc\n",
    "        torch.save(best_model.state_dict(), savepath_active)\n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "active_pbounds = {'lr': (1e-5, 0.01), 'model_index':(0, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=cnn_active_function,\n",
    "    pbounds=active_pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=25,   \n",
    ")\n",
    "end = time.time()\n",
    "print('Time consumed: ' , end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_idx = int(optimizer.max['params']['model_index']*num_ensemble)\n",
    "if model_idx == num_ensemble:\n",
    "    model_idx = num_ensemble-1\n",
    "\n",
    "active_final_model = MVTecCNN_BO(best_model_num_channels[model_idx]).to(device)\n",
    "active_final_model.load_state_dict(torch.load(savepath_active, map_location=device))\n",
    "\n",
    "active_model_test_acc = evaluate_accuracy(active_final_model.eval(), test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_best_normal_acc = 0\n",
    "savepath_normal = os.path.join(save_path, f\"normal_target-{target_class}_ensemble-{num_ensemble}_unlabel-{unlabeled_ratio}_annotate-{sample_ratio}.pth\")\n",
    "\n",
    "def cnn_normal_function(lr, model_index):\n",
    "    global global_best_normal_acc\n",
    "    if model_index==1:\n",
    "        idx=num_ensemble-1\n",
    "    else:\n",
    "        idx = int(model_index*num_ensemble)      \n",
    "    best_acc, best_model = activeTrain(idx, lr, isActiveLearn=False)\n",
    "    \n",
    "    if best_acc > global_best_normal_acc:\n",
    "        global_best_normal_acc = best_acc\n",
    "        torch.save(best_model.state_dict(), savepath_normal)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=cnn_normal_function,\n",
    "    pbounds=active_pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=25, \n",
    ")\n",
    "end = time.time()\n",
    "print('Time consumed: ' , end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_idx = int(optimizer.max['params']['model_index']*num_ensemble)\n",
    "if model_idx == num_ensemble:\n",
    "    model_idx = num_ensemble-1\n",
    "\n",
    "normal_final_model = MVTecCNN_BO(best_model_num_channels[model_idx]).to(device)\n",
    "normal_final_model.load_state_dict(torch.load(savepath_normal, map_location=device))\n",
    "\n",
    "normal_model_test_acc = evaluate_accuracy(normal_final_model.eval(), test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'original_model_test_acc: {original_model_test_acc} \\t active_model_test_acc:{active_model_test_acc} \\\n",
    "\\t normal_model_test_acc:{normal_model_test_acc}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
